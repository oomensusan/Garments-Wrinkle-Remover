{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce8f251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T18:34:02.140323Z",
     "iopub.status.busy": "2025-03-25T18:34:02.140004Z",
     "iopub.status.idle": "2025-03-25T18:34:02.145452Z",
     "shell.execute_reply": "2025-03-25T18:34:02.144482Z"
    },
    "papermill": {
     "duration": 0.010436,
     "end_time": "2025-03-25T18:34:02.146698",
     "exception": false,
     "start_time": "2025-03-25T18:34:02.136262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Coding\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6977aba0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-25T18:34:02.152148Z",
     "iopub.status.busy": "2025-03-25T18:34:02.151927Z",
     "iopub.status.idle": "2025-03-25T18:55:50.419268Z",
     "shell.execute_reply": "2025-03-25T18:55:50.418004Z"
    },
    "papermill": {
     "duration": 1308.271397,
     "end_time": "2025-03-25T18:55:50.420585",
     "exception": false,
     "start_time": "2025-03-25T18:34:02.149188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing images: 100%|██████████| 3022/3022 [10:48<00:00,  4.66image/s]\n",
      "Resizing images: 100%|██████████| 3022/3022 [10:58<00:00,  4.59image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 3022 images\n",
      "Failed to process: 0 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def resize_image(image, target_size=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Resize the image to a target size while maintaining aspect ratio.\n",
    "    Args:\n",
    "        image: Input image (numpy array).\n",
    "        target_size: Desired size (width, height).\n",
    "    Returns:\n",
    "        Resized image.\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    target_w, target_h = target_size\n",
    "\n",
    "    # Calculate the scaling factor\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_w = int(w * scale)\n",
    "    resized_h = int(h * scale)\n",
    "    resized_image = cv2.resize(image, (resized_w, resized_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Pad the image to the target size if necessary\n",
    "    delta_w = target_w - resized_w\n",
    "    delta_h = target_h - resized_h\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # Pad with black pixels (or any other color)\n",
    "    resized_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalize the image pixel values to the range [0, 1].\n",
    "    Args:\n",
    "        image: Input image (numpy array).\n",
    "    Returns:\n",
    "        Normalized image.\n",
    "    \"\"\"\n",
    "    return image / 255.0\n",
    "\n",
    "def compress_image(image, quality=90):\n",
    "    \"\"\"\n",
    "    Compress the image using JPEG compression.\n",
    "    Args:\n",
    "        image: Input image (numpy array).\n",
    "        quality: Compression quality (0-100).\n",
    "    Returns:\n",
    "        Compressed image.\n",
    "    \"\"\"\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "    _, compressed_image = cv2.imencode('.jpg', image, encode_param)\n",
    "    compressed_image = cv2.imdecode(compressed_image, cv2.IMREAD_COLOR)\n",
    "    return compressed_image\n",
    "\n",
    "def process_high_quality_image(image_path, output_dir, target_size=(1024, 1024), compress=True, quality=90):\n",
    "    \"\"\"\n",
    "    Process a high-quality image uniformly.\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        target_size: Desired size for resizing.\n",
    "        compress: Whether to compress the image.\n",
    "        quality: Compression quality (0-100).\n",
    "    Returns:\n",
    "        Processed image.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Supported image formats\n",
    "    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n",
    "    \n",
    "    # Get list of image files\n",
    "    image_files = [\n",
    "        f for f in os.listdir(image_path) \n",
    "        if os.path.splitext(f)[1].lower() in supported_formats\n",
    "    ]\n",
    "   \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    error_files = []\n",
    "\n",
    "    # Process each file in the input folder with progress bar\n",
    "    #for filename in tqdm(image_files[0:100], desc=\"Resizing images\", unit=\"image\"):\n",
    "    for filename in tqdm(image_files, desc=\"Resizing images\", unit=\"image\"):\n",
    "        file_path = os.path.join(image_path, filename)\n",
    "        # Load the image\n",
    "        image = cv2.imread(file_path)\n",
    "        #print(\"Image read completed\")\n",
    "        if image is None:\n",
    "            raise ValueError(\"Image not found or unable to load.\")\n",
    "    \n",
    "        # Resize the image\n",
    "        resized_image = resize_image(image, target_size)\n",
    "        #print(\"Image reszied\")\n",
    "    \n",
    "        # Normalize the image\n",
    "        normalized_image = normalize_image(resized_image)\n",
    "        # Convert back to [0, 255] range for saving (this is the key step you're missing)\n",
    "        image_to_save = (normalized_image * 255).astype(np.uint8)\n",
    "        #print(\"Image normalized\")\n",
    "    \n",
    "        # Compress the image (optional)\n",
    "        if compress:\n",
    "            #compressed_image = compress_image((normalized_image * 255).astype(np.uint8), quality)\n",
    "            image_to_save = compress_image(image_to_save, quality)\n",
    "            #return compressed_image\n",
    "\n",
    "        #return normalized_image\n",
    "        # Prepare output path\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        #print(output_path)\n",
    "        \n",
    "        # Save resized image\n",
    "        #compressed_image.save(output_path, quality=95, optimize=True)\n",
    "        #cv2.imwrite(output_path, normalized_image)\n",
    "        cv2.imwrite(output_path, image_to_save)\n",
    "        success_count += 1\n",
    "            \n",
    "    return success_count, error_count, error_files\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "#image_path = '/kaggle/input/wrinkled-cloth-dataset/CRHD-3K_src/CRHD-3K_src/004fa773a8a4e0bf8f6f8bf242f367329e0ced97.jpg'  # Replace with your image path\n",
    "#processed_image = process_high_quality_image(image_path, target_size=(512, 512), compress=True, quality=90)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example paths\n",
    "    input_dir_src = \"/kaggle/input/wrinkled-cloth-dataset/CRHD-3K_src/CRHD-3K_src\"\n",
    "    output_dir_src = \"./resized_images_input\"\n",
    "    input_dir_target = \"/kaggle/input/wrinkled-cloth-dataset/CRHD-3K_gt/CRHD-3K_gt\"\n",
    "    output_dir_target = \"./resized_images_output\"\n",
    "\n",
    "    img_size = 256\n",
    "    #Example with custom size\n",
    "    successes, errors, error_list = process_high_quality_image(\n",
    "        input_dir_src,\n",
    "        output_dir_src,\n",
    "        target_size=(img_size, img_size)\n",
    "    )\n",
    "\n",
    "    successes, errors, error_list = process_high_quality_image(\n",
    "        input_dir_target,\n",
    "        output_dir_target,\n",
    "        target_size=(img_size, img_size)\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully processed: {successes} images\")\n",
    "    print(f\"Failed to process: {errors} images\")\n",
    "    if error_list:\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for filename, error in error_list:\n",
    "            print(f\"- {filename}: {error}\")\n",
    "\n",
    "# Save or display the processed image\n",
    "#cv2.imwrite('processed_image.jpg', processed_image)\n",
    "#cv2.imshow('Processed Image', processed_image)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d311d610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T18:55:50.946071Z",
     "iopub.status.busy": "2025-03-25T18:55:50.945708Z",
     "iopub.status.idle": "2025-03-25T18:56:00.918228Z",
     "shell.execute_reply": "2025-03-25T18:56:00.917106Z"
    },
    "papermill": {
     "duration": 10.258897,
     "end_time": "2025-03-25T18:56:00.919719",
     "exception": false,
     "start_time": "2025-03-25T18:55:50.660822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete: 2417 training samples, 605 test samples\r\n",
      "Training dataset size: 2417\r\n",
      "Test dataset size: 605\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/input/train-inference/data_preparation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df530d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T18:56:01.398338Z",
     "iopub.status.busy": "2025-03-25T18:56:01.398035Z",
     "iopub.status.idle": "2025-03-25T18:56:01.401353Z",
     "shell.execute_reply": "2025-03-25T18:56:01.400679Z"
    },
    "papermill": {
     "duration": 0.241996,
     "end_time": "2025-03-25T18:56:01.402581",
     "exception": false,
     "start_time": "2025-03-25T18:56:01.160585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 /kaggle/input/train-inference/model_summary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843e33be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T18:56:01.876242Z",
     "iopub.status.busy": "2025-03-25T18:56:01.875934Z",
     "iopub.status.idle": "2025-03-25T21:21:25.215755Z",
     "shell.execute_reply": "2025-03-25T21:21:25.214615Z"
    },
    "papermill": {
     "duration": 8723.577289,
     "end_time": "2025-03-25T21:21:25.217440",
     "exception": false,
     "start_time": "2025-03-25T18:56:01.640151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/byrkbrk-model-data/diffusion_model.py:343: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(os.path.join(file_dir, \"checkpoints\", checkpoint_name),\r\n",
      "/kaggle/input/byrkbrk-model-data/diffusion_model.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(os.path.join(self.file_dir, \"checkpoints\", checkpoint_name), map_location=device)\r\n",
      "/kaggle/input/byrkbrk-model-data/diffusion_model.py:350: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  batch_size = torch.load(os.path.join(file_dir, \"checkpoints\", checkpoint_name),\r\n",
      "/kaggle/input/byrkbrk-model-data/diffusion_model.py:283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(os.path.join(file_dir, \"checkpoints\", checkpoint_name), map_location=device)\r\n",
      "/kaggle/input/byrkbrk-model-data/diffusion_model.py:292: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(os.path.join(file_dir, \"checkpoints\", checkpoint_name), map_location=device)\r\n",
      "/kaggle/input/byrkbrk-model-data/diffusion_model.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  start_epoch = torch.load(os.path.join(file_dir, \"checkpoints\", checkpoint_name),\r\n",
      "Epoch 84: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 84, loss: 0.007307925213833077\r\n",
      "Epoch 85: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 85, loss: 0.005870756481453401\r\n",
      "Epoch 86: 100%|█████████████████████████████| 1209/1209 [06:50<00:00,  2.94it/s]\r\n",
      "Epoch: 86, loss: 0.00662998215364126\r\n",
      "Epoch 87: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 87, loss: 0.00594129090272875\r\n",
      "Epoch 88: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 88, loss: 0.00652141992796739\r\n",
      "Epoch 89: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 89, loss: 0.006663528756709194\r\n",
      "Epoch 90: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 90, loss: 0.006841544492895861\r\n",
      "Epoch 91: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 91, loss: 0.0065920368232513975\r\n",
      "Epoch 92: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 92, loss: 0.007002097864548529\r\n",
      "Epoch 93: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 93, loss: 0.0067787532041619\r\n",
      "Epoch 94: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 94, loss: 0.006173274072749984\r\n",
      "Epoch 95: 100%|█████████████████████████████| 1209/1209 [06:50<00:00,  2.94it/s]\r\n",
      "Epoch: 95, loss: 0.006328090808241276\r\n",
      "Epoch 96: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 96, loss: 0.005906069650475741\r\n",
      "Epoch 97: 100%|█████████████████████████████| 1209/1209 [06:50<00:00,  2.94it/s]\r\n",
      "Epoch: 97, loss: 0.00685352056926454\r\n",
      "Epoch 98: 100%|█████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 98, loss: 0.0058251245388108545\r\n",
      "Epoch 99: 100%|█████████████████████████████| 1209/1209 [06:50<00:00,  2.94it/s]\r\n",
      "Epoch: 99, loss: 0.00618168076554845\r\n",
      "Epoch 100: 100%|████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 100, loss: 0.006177952959850945\r\n",
      "Epoch 101: 100%|████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 101, loss: 0.006656639970926354\r\n",
      "Epoch 102: 100%|████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 102, loss: 0.00607667175413655\r\n",
      "Epoch 103: 100%|████████████████████████████| 1209/1209 [06:51<00:00,  2.94it/s]\r\n",
      "Epoch: 103, loss: 0.0065383433242139\r\n",
      "Epoch 104: 100%|████████████████████████████| 1209/1209 [06:50<00:00,  2.94it/s]\r\n",
      "Epoch: 104, loss: 0.006307014246027603\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/input/byrkbrk-model-data/train.py --dataset-name custom_data --batch-size 2 --epochs 21 --checkpoint-name custom_data_checkpoint_83.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8f3c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T21:21:26.094363Z",
     "iopub.status.busy": "2025-03-25T21:21:26.094066Z",
     "iopub.status.idle": "2025-03-25T21:21:26.097540Z",
     "shell.execute_reply": "2025-03-25T21:21:26.096942Z"
    },
    "papermill": {
     "duration": 0.447537,
     "end_time": "2025-03-25T21:21:26.098833",
     "exception": false,
     "start_time": "2025-03-25T21:21:25.651296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 /kaggle/input/byrkbrk-model-data/sample.py pretrained_fashion_mnist_checkpoint_49.pth --n-samples 400 --n-images-per-row 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bfa705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T21:21:27.089844Z",
     "iopub.status.busy": "2025-03-25T21:21:27.089345Z",
     "iopub.status.idle": "2025-03-25T21:21:27.093355Z",
     "shell.execute_reply": "2025-03-25T21:21:27.092675Z"
    },
    "papermill": {
     "duration": 0.484539,
     "end_time": "2025-03-25T21:21:27.094450",
     "exception": false,
     "start_time": "2025-03-25T21:21:26.609911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_feature_target(self, target_img, feature_threshold=0.1, edge_weight=0.7, detail_weight=0.3):\n",
    "#     \"\"\"\n",
    "#     Extract and preserve main features from the target image\n",
    "    \n",
    "#     Args:\n",
    "#         target_img: Input target image tensor [B, C, H, W]\n",
    "#         feature_threshold: Threshold for feature importance (lower = more features)\n",
    "#         edge_weight: Weight given to edge features\n",
    "#         detail_weight: Weight given to detail features\n",
    "    \n",
    "#     Returns:\n",
    "#         Processed target image with main features preserved\n",
    "#     \"\"\"\n",
    "#     batch_size = target_img.shape[0]\n",
    "#     device = target_img.device\n",
    "    \n",
    "#     # Create output tensor\n",
    "#     feature_target = torch.zeros_like(target_img)\n",
    "    \n",
    "#     for i in range(batch_size):\n",
    "#         img = target_img[i].clone()  # [C, H, W]\n",
    "        \n",
    "#         # 1. Extract edges using Sobel filters\n",
    "#         sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=device)\n",
    "#         sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=device)\n",
    "        \n",
    "#         # Process each channel\n",
    "#         edge_map = torch.zeros_like(img[0])  # [H, W]\n",
    "#         for c in range(img.shape[0]):\n",
    "#             # Apply sobel filters using conv2d\n",
    "#             padded = F.pad(img[c].unsqueeze(0).unsqueeze(0), (1, 1, 1, 1), mode='reflect')\n",
    "#             gx = F.conv2d(padded, sobel_x.unsqueeze(0).unsqueeze(0))\n",
    "#             gy = F.conv2d(padded, sobel_y.unsqueeze(0).unsqueeze(0))\n",
    "#             edge_strength = torch.sqrt(gx**2 + gy**2).squeeze()\n",
    "#             edge_map += edge_strength\n",
    "        \n",
    "#         # Normalize edge map\n",
    "#         if edge_map.max() > 0:\n",
    "#             edge_map = edge_map / edge_map.max()\n",
    "        \n",
    "#         # 2. Extract high-frequency details using Laplacian filter\n",
    "#         laplacian = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=torch.float32, device=device)\n",
    "        \n",
    "#         detail_map = torch.zeros_like(img[0])  # [H, W]\n",
    "#         for c in range(img.shape[0]):\n",
    "#             padded = F.pad(img[c].unsqueeze(0).unsqueeze(0), (1, 1, 1, 1), mode='reflect')\n",
    "#             detail = F.conv2d(padded, laplacian.unsqueeze(0).unsqueeze(0)).abs().squeeze()\n",
    "#             detail_map += detail\n",
    "        \n",
    "#         # Normalize detail map\n",
    "#         if detail_map.max() > 0:\n",
    "#             detail_map = detail_map / detail_map.max()\n",
    "        \n",
    "#         # 3. Combine edge and detail maps\n",
    "#         feature_map = edge_weight * edge_map + detail_weight * detail_map\n",
    "        \n",
    "#         # 4. Apply threshold to create binary feature mask\n",
    "#         feature_mask = (feature_map > feature_threshold).float()\n",
    "        \n",
    "#         # 5. Apply mask to preserve main features\n",
    "#         # Expand mask to match channels\n",
    "#         feature_mask = feature_mask.unsqueeze(0).expand_as(img)\n",
    "#         feature_target[i] = img * feature_mask\n",
    "    \n",
    "#     return feature_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb32bc",
   "metadata": {
    "papermill": {
     "duration": 0.44082,
     "end_time": "2025-03-25T21:21:27.975856",
     "exception": false,
     "start_time": "2025-03-25T21:21:27.535036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6621932,
     "sourceId": 10687845,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6837170,
     "sourceId": 11098306,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6940167,
     "sourceId": 11129141,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6954388,
     "sourceId": 11147508,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6946388,
     "sourceId": 11164617,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10049.560734,
   "end_time": "2025-03-25T21:21:29.022796",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-25T18:33:59.462062",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
